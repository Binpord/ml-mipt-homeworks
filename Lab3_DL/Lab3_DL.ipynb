{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лабораторная работа №3.\n",
    "\n",
    "Дедлайн: 21 мая 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 1. Overfit it (1.5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем работать с датасетом [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) (*hint: он доступен в torchvision*).\n",
    "\n",
    "Ваша задача состоит в следующем:\n",
    "1. Обучить сеть, которая покажет >= 0.94 test accuracy.\n",
    "2. Пронаблюдать и продемонстрировать процесс переобучения сети с увеличением числа параметров (==нейронов) и продемонстрировать это наглядно (например, на графиках).\n",
    "3. Попробовать частично справиться с переобучением с помощью подходящих приемов (Dropout/batchnorm/augmentation etc.)\n",
    "\n",
    "*Примечание*: Пункты 2 и 3 взаимосвязаны, в п.3 Вам прелагается сделать полученную в п.2 сеть менее склонной к переобучению. Пункт 1 является независимым от пунктов 2 и 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T19:08:19.040985Z",
     "start_time": "2019-05-19T19:08:18.923801Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import FashionMNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "train = FashionMNIST(root='./data', download=True, train=True, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: F.pad(x.unsqueeze(0), (4,4,4,4),mode='reflect').squeeze()),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]], \n",
    "                         std=[x/255.0 for x in [63.0, 62.1, 66.7]]),\n",
    "]))\n",
    "X_train, y_train = train.data.view(-1, 1, 28, 28).float(), train.targets\n",
    "\n",
    "test = FashionMNIST(root='./data', download=True, train=False, transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]], \n",
    "                         std=[x/255.0 for x in [63.0, 62.1, 66.7]]),\n",
    "]))\n",
    "X_test, y_test = test.data.view(-1, 1, 28, 28).float(), test.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T19:08:19.626892Z",
     "start_time": "2019-05-19T19:08:19.619557Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool = nn.MaxPool2d(2, 2, 0)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6 ,16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 400)\n",
    "        self.fc2 = nn.Linear(400, 200)\n",
    "        self.fc3 = nn.Linear(200, len(train.classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T19:10:26.815531Z",
     "start_time": "2019-05-19T19:08:21.243292Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.610206127166748\n",
      "1 3.1224687099456787\n",
      "2 2.492177963256836\n",
      "3 2.211887836456299\n",
      "4 2.209336519241333\n",
      "5 2.0669403076171875\n",
      "6 1.810515284538269\n",
      "7 1.6081788539886475\n",
      "8 1.4536948204040527\n",
      "9 1.2901382446289062\n",
      "10 1.1281310319900513\n",
      "11 1.0111831426620483\n",
      "12 0.9542237520217896\n",
      "13 0.9729079008102417\n",
      "14 1.0009342432022095\n",
      "15 0.9532195925712585\n",
      "16 0.869604229927063\n",
      "17 0.8125401139259338\n",
      "18 0.7891591787338257\n",
      "19 0.7875454425811768\n",
      "20 0.7949892282485962\n",
      "21 0.7916980981826782\n",
      "22 0.7703601717948914\n",
      "23 0.7435097694396973\n",
      "24 0.7219945788383484\n",
      "25 0.7068776488304138\n",
      "26 0.697105348110199\n",
      "27 0.692058265209198\n",
      "28 0.6874521374702454\n",
      "29 0.6770157814025879\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "for t in range(30):\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                               padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
    "        assert((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(1, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(depth=10, num_classes = len(train.targets))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "for t in range(30):\n",
    "    y_pred = model(train.data.view(-1, 1, 28, 28).float())\n",
    "    loss = criterion(y_pred, train.targets)\n",
    "    print(t, loss.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T19:24:50.582405Z",
     "start_time": "2019-05-19T19:24:50.262240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 74.53\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    out = model(test.data.view(-1, 1, 28, 28).float())\n",
    "    _, pred = torch.max(out.data, 1)\n",
    "    correct += (pred == test.targets).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / test.targets.shape[0]\n",
    "print(f'Test accuracy is {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Часть 1. Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала загрузим данные. Файл с сонетами Шекспира доступен по [ссылке](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). Кроме того, он находится рядом с этим ноутбуком (`sonnetes.txt`).\n",
    "\n",
    "Базовая предобработка уже сделана: текст состоит непосредственно из поэм Шекспира и названий/номеров глав, все техническая информация удалена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sonnets.txt', 'r') as iofile:\n",
    "    text = iofile.readlines()\n",
    "    \n",
    "TEXT_START = 45\n",
    "TEXT_END = -368\n",
    "text = text[TEXT_START:TEXT_END]\n",
    "assert len(text) == 2616"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как в этот раз мы хотим научиться предсказывать текст, понизим сложность задачи и приведем текст к нижнему регистру.\n",
    "\n",
    "В настоящий момент переменная `text` представляет собой список из строк. Объедините все строки в одну и приведите к нижнему регистру. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объедините все строки в одну и приведите к нижнему регистру.\n",
    "# Результат запишите в переменную text.\n",
    "\n",
    "# Your great code here\n",
    "\n",
    "assert len(text) == 100225, 'Are you sure you have concatenated all the strings?'\n",
    "assert not any([x in set(text) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
    "print('Отлично!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделите множество всех символов, с которыми нам довелось встретиться в переменную `tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = sorted(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте словарь `token_to_idx` вида <символ>: <индекс> и словарь `idx_to_token` вида <индекс>: <символ>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# словарь вида <индекс>:<символ>\n",
    "# Your great code here\n",
    "\n",
    "# словарь вида <символ>:<индекс>\n",
    "# Your great code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Комментарий: т.к. у нас всего 38 различных токенов, в этот раз воспользуемся one-hot encoding'ом.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь наша задача - создать и обучить рекуррентную нейронную сеть, которая сможет генерировать что-то похожее на поэзию Шекспира.\n",
    "\n",
    "Для начала воспользуемся классической RNN, аналогичной построенной на семинаре. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your modified code from class here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте график функции потерь в зависимости от номера эпохи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your plot code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример сгенерированного текста. Функция `generate_text` отсутствует в коде выше.\n",
    "# print(generate_text(length=500, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Более поэтичная модель\n",
    "\n",
    "Теперь давайте воспользуемся LSTM слоем вместо классической RNN и сравним результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Снова постройте график функции потерь от числа эпох. Стал ли финальный loss лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your beautiful code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируйте текст с помощью обученной сети для различных значений параметра `temperature`: `[0.1, 0.2, 0.5, 1.0, 2.0]` (\"температуры\" при генерации). Оцените результаты визуально, попробуйте их проинтерпретировать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text generation with different tempearature values here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь можно оставить свои рассуждения касательно интерпретации результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Сохранение и загрузка модели\n",
    "\n",
    "Сохраните обученную модель на диск, затем загрузите ее и сгенерируйте текст. Примеры доступны по [ссылке](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and loading code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная часть задания завершена."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Полезные ссылки\n",
    "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Статья Андрея Карпатого про RNN. </a> В качестве примеров рассматриваются задачи генерации Шекспировских текстов, Latex формул, Linux Source Code и детских имен.\n",
    "2. <a href='https://github.com/karpathy/char-rnn'> Репозиторий с кодом по char-rnn </a> (тоже за авторством Андрея Карпатого)\n",
    "3. Полезный репозиторий по PyTorch: [ссылка](https://github.com/spro/practical-pytorch`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________\n",
    "____________________\n",
    "____________________\n",
    "____________________\n",
    "____________________\n",
    "____________________\n",
    "____________________\n",
    "____________________\n",
    "____________________\n",
    "____________________\n",
    "____________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Части задания 3 (HAR classification) и 4 (kaggle) доступны на выбор. Вы можете выполнить одну из них, или обе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 3. HAR classification with raw data (2+ points)\n",
    "__Disclaimer__: В данном задании придется экспериментировать, подбирать оптимальную структуру сети для решения задачи и активно искать подскзаки в сети.\n",
    "\n",
    "\n",
    "Данное задание составлено на основе данного [поста](https://burakhimmetoglu.com/2017/08/22/time-series-classification-with-tensorflow/). С помощью вручную сгенерированных фичей и классических подходов задача распознования движений была решена с точностью 96%. \n",
    "\n",
    "Также будет полезным изучить [вот этот](https://github.com/healthDataScience/deep-learning-HAR), а так же [вот этот репозиторий](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition), где к данной задаче рассматривается несколько подходов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import pylab\n",
    "import warnings as w\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size':14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вернемся к задаче классификации движений на основе [данных](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) из репозитория UCI ([прямая ссылка на скачивание](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip)). \n",
    "\n",
    "В этот раз будем работать с исходными, а не предобработанными данными. Данные представляют собой сигналы с гироскопа и акселерометра, закрепленного на теле человека. Каждому семплу соотвествует 9 связанных временных рядов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале приведена визуализация данных на основе PCA над вручную сгенерированными признаками. Для отрисовки графиков (цвет и легенда) нам также понадобятся метки классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_with_engineered_features = np.genfromtxt(os.path.join(\"UCI HAR Dataset\", \"train\", \"X_train.txt\"))\n",
    "y_train = np.genfromtxt(os.path.join(\"UCI HAR Dataset\", \"train\", \"y_train.txt\"))\n",
    "\n",
    "y_train_list = list(y_train)\n",
    "X_unique = np.array([X_train_with_engineered_features[y_train_list.index(l)]\n",
    "                             for l in sorted(list(set(y_train)))])\n",
    "\n",
    "legend_labels = [\"WALKING\", \"WALKING.UP\", \"WALKING.DOWN\", \"SITTING\", \"STANDING\", \"LAYING\"]\n",
    "colors_list = ['red', 'blue', 'green', 'orange', 'cyan', 'magenta']\n",
    "mapped_colors = [colors_list[int(i)-1] for i in y_train]\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_with_engineered_features)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "pylab.scatter(X_train_pca[:, 0], X_train_pca[:, 1],\n",
    "             c=mapped_colors)\n",
    "plt.grid()\n",
    "for idx, x in enumerate(pca.transform(X_unique)):\n",
    "    plt.scatter(x[0], \n",
    "                x[1], \n",
    "                c=colors_list[idx], \n",
    "                label=legend_labels[idx])\n",
    "plt.xlabel('First principal component')\n",
    "plt.ylabel('Second principal component')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Предобработка данных\n",
    "Предобработка сделана за нас автором [данного репозитория](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition). Будьте осторожны с путями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "\n",
    "# Those are separate normalised input features for the neural network\n",
    "INPUT_SIGNAL_TYPES = [\n",
    "    \"body_acc_x_\",\n",
    "    \"body_acc_y_\",\n",
    "    \"body_acc_z_\",\n",
    "    \"body_gyro_x_\",\n",
    "    \"body_gyro_y_\",\n",
    "    \"body_gyro_z_\",\n",
    "    \"total_acc_x_\",\n",
    "    \"total_acc_y_\",\n",
    "    \"total_acc_z_\"\n",
    "]\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "LABELS = [\n",
    "    \"WALKING\", \n",
    "    \"WALKING_UPSTAIRS\", \n",
    "    \"WALKING_DOWNSTAIRS\", \n",
    "    \"SITTING\", \n",
    "    \"STANDING\", \n",
    "    \"LAYING\"\n",
    "]\n",
    "\n",
    "DATA_PATH = \"./\"\n",
    "\n",
    "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
    "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n",
    "\n",
    "TRAIN = \"train/\"\n",
    "TEST = \"test/\"\n",
    "\n",
    "\n",
    "# Load \"X\" (the neural network's training and testing inputs)\n",
    "\n",
    "def load_X(X_signals_paths):\n",
    "    X_signals = []\n",
    "    \n",
    "    for signal_type_path in X_signals_paths:\n",
    "        file = open(signal_type_path, 'r')\n",
    "        # Read dataset from disk, dealing with text files' syntax\n",
    "        X_signals.append(\n",
    "            [np.array(serie, dtype=np.float32) for serie in [\n",
    "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "            ]]\n",
    "        )\n",
    "        file.close()\n",
    "    \n",
    "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
    "\n",
    "X_train_signals_paths = [\n",
    "    os.path.join(*[DATASET_PATH, TRAIN, \"Inertial Signals/\", signal+\"train.txt\"]) for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "X_test_signals_paths = [\n",
    "    os.path.join(*[DATASET_PATH, TEST, \"Inertial Signals/\", signal+\"test.txt\"]) for signal in INPUT_SIGNAL_TYPES\n",
    "]\n",
    "\n",
    "X_train = load_X(X_train_signals_paths)\n",
    "X_test = load_X(X_test_signals_paths)\n",
    "\n",
    "\n",
    "# Load \"y\" (the neural network's training and testing outputs)\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    # Read dataset from disk, dealing with text file's syntax\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # Substract 1 to each output class for friendly 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "y_train_path = os.path.join(DATASET_PATH, TRAIN, \"y_train.txt\")\n",
    "y_test_path = os.path.join(DATASET_PATH, TEST, \"y_test.txt\")\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 2947 testing series\n",
    "n_steps = len(X_train[0])  # 128 timesteps per series\n",
    "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
    "\n",
    "\n",
    "# LSTM Neural Network's internal structure\n",
    "\n",
    "n_hidden = 32 # Hidden layer num of features\n",
    "n_classes = 6 # Total classes (should go up, or should go down)\n",
    "\n",
    "\n",
    "# Some debugging info\n",
    "\n",
    "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Построение сети и эксперименты. (100% +)\n",
    "\n",
    "__Ваша задача - построить сеть, которая решит задачу классификации с точностью (`accuracy`) не менее 85%.__\n",
    "Разбалловка следующая:\n",
    "* $=$85% - 2 points\n",
    "* $>=$89% - 2.5 points\n",
    "* $>=$91% - 3 points\n",
    "\n",
    "\n",
    "__Warning!__ В сети существует несколько решений данной задачи с использованием различных фреймворков. При проверке это будет учитываться, так что свое решение нужно будет объяснить. Пожалуйста, не копируйте бездумно код, такие задания будут оценены 0 баллов. Если задача не решается - можете обратиться к заданию по классификации изображений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После выполнения задания заполните небольшой отчет об экспериментах вида \"Я пробовал(а) ... подходы и получил(а) ... результаты. Наконец, после N+1 чашки кофе/бессонной ночи у меня получилось, и весь секрет был в ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your experiments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Часть 4. Dogs classification.\n",
    "\n",
    "Предлагаем вам решить задачу классификации пород собак. Вы можете обучить сеть с нуля или же воспользоваться методом fine-tuning'а. Полезная ссылка на [предобученные модели](https://pytorch.org/docs/stable/torchvision/models.html).\n",
    "\n",
    "Данные можно скачать [отсюда](https://www.dropbox.com/s/vgqpz2f1lolxmlv/data.zip?dl=0). Датасет представлен 50 классами пород собак, которые можно найти в папке train в соответствующих директориях. При сдаче данной части задания вместе с ноутбуком необходимо отправить .csv-файл с предсказаниями классов тестовой выборки в формате: <имя изображения>,<метка класса> по одному объекту на строку. Ниже приведите код ваших экспериментов и короткий вывод по их результатам.\n",
    "\n",
    "Будут оцениваться качество классификации (accuracy) на тестовой выборке (2 балла) и проведенные эксперименты (1 балл).\n",
    "Разбалловка следующая:\n",
    "* $>=$96% - 2 points\n",
    "* $>=$85% - 1.5 points\n",
    "* $>=$70% - 0.75 points"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
